2018-10-08 13:28:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-10-08 13:28:13 INFO  HiveMetaStore:589 - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-10-08 13:28:13 INFO  ObjectStore:289 - ObjectStore, initialize called
2018-10-08 13:28:14 INFO  Persistence:77 - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2018-10-08 13:28:14 INFO  Persistence:77 - Property datanucleus.cache.level2 unknown - will be ignored
2018-10-08 13:28:15 INFO  ObjectStore:370 - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2018-10-08 13:28:15 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2018-10-08 13:28:15 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2018-10-08 13:28:15 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2018-10-08 13:28:15 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2018-10-08 13:28:16 INFO  Query:77 - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2018-10-08 13:28:16 INFO  MetaStoreDirectSql:139 - Using direct SQL, underlying DB is DERBY
2018-10-08 13:28:16 INFO  ObjectStore:272 - Initialized ObjectStore
2018-10-08 13:28:16 INFO  HiveMetaStore:663 - Added admin role in metastore
2018-10-08 13:28:16 INFO  HiveMetaStore:672 - Added public role in metastore
2018-10-08 13:28:16 INFO  HiveMetaStore:712 - No user is added in admin role, since config is empty
2018-10-08 13:28:16 INFO  HiveMetaStore:746 - 0: get_all_databases
2018-10-08 13:28:16 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=get_all_databases	
2018-10-08 13:28:16 INFO  HiveMetaStore:746 - 0: get_functions: db=default pat=*
2018-10-08 13:28:16 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2018-10-08 13:28:16 INFO  Datastore:77 - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
2018-10-08 13:28:16 INFO  HiveMetaStore:746 - 0: get_functions: db=tpcds pat=*
2018-10-08 13:28:16 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=get_functions: db=tpcds pat=*	
2018-10-08 13:28:16 INFO  SessionState:641 - Created local directory: /var/folders/z2/52r6zpqx4g79gdsyz01h48_w0000gn/T/939b843e-dd5b-402b-a369-2ced09d6b3e9_resources
2018-10-08 13:28:16 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/edoardo/939b843e-dd5b-402b-a369-2ced09d6b3e9
2018-10-08 13:28:16 INFO  SessionState:641 - Created local directory: /var/folders/z2/52r6zpqx4g79gdsyz01h48_w0000gn/T/edoardo/939b843e-dd5b-402b-a369-2ced09d6b3e9
2018-10-08 13:28:16 INFO  SessionState:641 - Created HDFS directory: /tmp/hive/edoardo/939b843e-dd5b-402b-a369-2ced09d6b3e9/_tmp_space.db
2018-10-08 13:28:16 INFO  SparkContext:54 - Running Spark version 2.3.2
2018-10-08 13:28:16 INFO  SparkContext:54 - Submitted application: SparkSQL::192.168.0.101
2018-10-08 13:28:16 INFO  SecurityManager:54 - Changing view acls to: edoardo
2018-10-08 13:28:16 INFO  SecurityManager:54 - Changing modify acls to: edoardo
2018-10-08 13:28:16 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-10-08 13:28:16 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-10-08 13:28:16 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edoardo); groups with view permissions: Set(); users  with modify permissions: Set(edoardo); groups with modify permissions: Set()
2018-10-08 13:28:17 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 52304.
2018-10-08 13:28:17 INFO  SparkEnv:54 - Registering MapOutputTracker
2018-10-08 13:28:17 INFO  SparkEnv:54 - Registering BlockManagerMaster
2018-10-08 13:28:17 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-10-08 13:28:17 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2018-10-08 13:28:17 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/z2/52r6zpqx4g79gdsyz01h48_w0000gn/T/blockmgr-ee22428d-36a8-4e7b-a023-9327d6d19ce0
2018-10-08 13:28:17 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2018-10-08 13:28:17 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2018-10-08 13:28:17 INFO  log:192 - Logging initialized @5801ms
2018-10-08 13:28:17 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2018-10-08 13:28:17 INFO  Server:419 - Started @5920ms
2018-10-08 13:28:17 INFO  AbstractConnector:278 - Started ServerConnector@309dcdf3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-10-08 13:28:17 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3dce6dd8{/jobs,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a950fdd{/jobs/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77724cbe{/jobs/job,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@570ba13{/jobs/job/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37a9b687{/stages,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@525b1b70{/stages/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16d07cf3{/stages/stage,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@661d6bb6{/stages/stage/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@733fb462{/stages/pool,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@623e0631{/stages/pool/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@359066bc{/storage,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@385dfb63{/storage/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@364fd4ae{/storage/rdd,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@245253d8{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@12417468{/environment,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@459003a0{/environment/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@d325518{/executors,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3b481bf5{/executors/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2233cac0{/executors/threadDump,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@67fb5025{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@787e4357{/static,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e8b357d{/,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e1eb85f{/api,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@31723307{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2401856{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-10-08 13:28:17 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.101:4040
2018-10-08 13:28:17 INFO  Executor:54 - Starting executor ID driver on host localhost
2018-10-08 13:28:17 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52305.
2018-10-08 13:28:17 INFO  NettyBlockTransferService:54 - Server created on 192.168.0.101:52305
2018-10-08 13:28:17 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-10-08 13:28:17 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.0.101, 52305, None)
2018-10-08 13:28:17 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.0.101:52305 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.101, 52305, None)
2018-10-08 13:28:17 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.0.101, 52305, None)
2018-10-08 13:28:17 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.0.101, 52305, None)
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@435e416c{/metrics/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/').
2018-10-08 13:28:18 INFO  SharedState:54 - Warehouse path is 'file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/'.
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@543fe698{/SQL,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b2fdffc{/SQL/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@637c840d{/SQL/execution,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@51ac12ac{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5c3d4f05{/static/sql,null,AVAILABLE,@Spark}
2018-10-08 13:28:18 INFO  HiveUtils:54 - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-10-08 13:28:18 INFO  HiveClientImpl:54 - Warehouse location for Hive client (version 1.2.2) is file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/
2018-10-08 13:28:18 INFO  metastore:291 - Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/
2018-10-08 13:28:18 INFO  HiveMetaStore:746 - 0: Shutting down the object store...
2018-10-08 13:28:18 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=Shutting down the object store...	
2018-10-08 13:28:18 INFO  HiveMetaStore:746 - 0: Metastore shutdown complete.
2018-10-08 13:28:18 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
2018-10-08 13:28:18 INFO  HiveMetaStore:746 - 0: get_database: default
2018-10-08 13:28:18 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=get_database: default	
2018-10-08 13:28:18 INFO  HiveMetaStore:589 - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-10-08 13:28:18 INFO  ObjectStore:289 - ObjectStore, initialize called
2018-10-08 13:28:18 INFO  Query:77 - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
2018-10-08 13:28:18 INFO  MetaStoreDirectSql:139 - Using direct SQL, underlying DB is DERBY
2018-10-08 13:28:18 INFO  ObjectStore:272 - Initialized ObjectStore
2018-10-08 13:28:19 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2018-10-08 13:28:19 INFO  HiveMetaStore:746 - 0: get_database: global_temp
2018-10-08 13:28:19 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=get_database: global_temp	
2018-10-08 13:28:19 WARN  ObjectStore:568 - Failed to get database global_temp, returning NoSuchObjectException
2018-10-08 13:28:21 INFO  HiveMetaStore:746 - 0: create_database: Database(name:tpcds_db, description:For TPCDS at 1GB scale factor, locationUri:file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/tpcds_db.db, parameters:{})
2018-10-08 13:28:21 INFO  audit:371 - ugi=edoardo	ip=unknown-ip-addr	cmd=create_database: Database(name:tpcds_db, description:For TPCDS at 1GB scale factor, locationUri:file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/tpcds_db.db, parameters:{})	
2018-10-08 13:28:21 WARN  ObjectStore:568 - Failed to get database tpcds_db, returning NoSuchObjectException
2018-10-08 13:28:21 INFO  FileUtils:501 - Creating directory if it doesn't exist: file:/Users/edoardo/Documents/BDMA/Data_Warehouses/Project/spark/spark-warehouse/tpcds_db.db
2018-10-08 13:28:21 INFO  SparkSQLCLIDriver:951 - Time taken: 1.961 seconds
2018-10-08 13:28:21 INFO  AbstractConnector:318 - Stopped Spark@309dcdf3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-10-08 13:28:21 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.0.101:4040
2018-10-08 13:28:21 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2018-10-08 13:28:21 INFO  MemoryStore:54 - MemoryStore cleared
2018-10-08 13:28:21 INFO  BlockManager:54 - BlockManager stopped
2018-10-08 13:28:21 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2018-10-08 13:28:21 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2018-10-08 13:28:21 INFO  SparkContext:54 - Successfully stopped SparkContext
2018-10-08 13:28:21 INFO  ShutdownHookManager:54 - Shutdown hook called
2018-10-08 13:28:21 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/z2/52r6zpqx4g79gdsyz01h48_w0000gn/T/spark-7487ee81-e9fb-48c2-9b0d-eeefa4724bb5
2018-10-08 13:28:21 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/z2/52r6zpqx4g79gdsyz01h48_w0000gn/T/spark-5fb02241-da6a-4134-99f2-7b3fedb767e6
